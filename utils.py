#!/usr/bin/env python3
"""
Utility functions for analyzing text files and estimating whether content was
generated by an AI or written by a human.  The heuristics implemented here
are intentionally simple and based on publicly documented readability
formulas and syllable‑counting rules so the logic is transparent.  The
Flesch reading‑ease formula compares average sentence length and average
syllables per word to compute a score indicating how easy the text is to
understand.  Higher scores correspond to easier reading and lower values
indicate more complex language【337667417158112†L169-L175】.  To count
syllables we use a straightforward “written method” that counts vowel
clusters, treats the letter “y” as a vowel in some contexts, subtracts
silent vowels at the end of words and diphthongs, and adds a syllable
when a word ends in “le” after a consonant【350105103423052†L104-L112】.

These functions do not rely on any third‑party libraries; they operate
entirely on built‑in Python modules and rudimentary heuristics.  They
should therefore run in restricted environments without external
dependencies.  They are not intended to provide definitive AI detection
but instead to illustrate how one might implement simple metrics that
could feed into more sophisticated classifiers.
"""

import os
import re
import string
from typing import Dict, Tuple


def count_syllables(word: str) -> int:
    """Estimate the number of syllables in a single English word.

    This implementation follows the "written method" rules described in
    guidance on counting syllables【350105103423052†L104-L112】.  It counts
    vowel occurrences, treats ‘y’ as a vowel when it functions as one,
    subtracts one for a silent ‘e’ at the end of a word and for each
    contiguous vowel group (diphthong/triphthong), and adds a syllable
    when a word ends in “le” or “les” preceded by a consonant.  The
    result is at least one syllable.

    Args:
        word: A string containing the word to analyse (case insensitive).

    Returns:
        An integer estimate of the number of syllables in the word.
    """
    word = word.lower().strip()
    if not word:
        return 0
    # Define vowels; treat 'y' as a vowel when appropriate
    vowels = "aeiou"
    syllables = 0
    prev_char_was_vowel = False
    for i, char in enumerate(word):
        # consider y as vowel when previous char is not a vowel
        is_vowel = char in vowels or (char == 'y' and i > 0 and word[i - 1] not in vowels)
        if is_vowel and not prev_char_was_vowel:
            syllables += 1
        prev_char_was_vowel = is_vowel
    # subtract one syllable for silent trailing 'e'
    if word.endswith('e') and syllables > 1:
        syllables -= 1
    # subtract one for each diphthong/triphthong (two consecutive vowels)
    # because contiguous vowels were counted separately above
    for diph in ["ai", "au", "ay", "ea", "ee", "ei", "eu", "ey", "ie", "io",
                 "iu", "oa", "oe", "oi", "oo", "ou", "oy", "ue", "ui", "ye", "yi"]:
        if diph in word:
            syllables -= word.count(diph)
    # add one when word ends with "le" or "les" after a consonant
    if word.endswith("le") and len(word) > 2 and word[-3] not in vowels:
        syllables += 1
    if word.endswith("les") and len(word) > 3 and word[-4] not in vowels:
        syllables += 1
    if syllables <= 0:
        syllables = 1
    return syllables


def split_into_sentences(text: str) -> list:
    """Split text into sentences using basic punctuation heuristics."""
    # Use regex to split on sentence ending punctuation followed by space
    sentences = re.split(r'[.!?]+\s+', text.strip())
    # Remove any empty strings from list
    return [s for s in sentences if s]


def tokenize(text: str) -> list:
    """Tokenize text into words, removing punctuation."""
    # Replace hyphens with spaces to avoid joining words
    text = text.replace('-', ' ')
    # Remove punctuation using str.translate
    translator = str.maketrans('', '', string.punctuation)
    cleaned = text.translate(translator)
    return [w for w in cleaned.split() if w]


def compute_flesch_reading_ease(word_count: int, sentence_count: int, syllable_count: int) -> float:
    """Calculate the Flesch reading‑ease score.【337667417158112†L169-L175】"""
    if word_count == 0 or sentence_count == 0:
        return 0.0
    # Avoid division by zero
    words_per_sentence = word_count / sentence_count
    syllables_per_word = syllable_count / word_count
    return 206.835 - 1.015 * words_per_sentence - 84.6 * syllables_per_word


def analyse_text_content(text: str) -> Dict[str, float]:
    """Compute readability, diversity and AI‑detection metrics for the given text.

    This function computes several basic statistics about a body of text and then
    derives two AI‑related signals.  The first is a simple likelihood based on
    lexical diversity and readability; the second uses a more rigorous set of
    features including lexical repetition, n‑gram diversity, function‑word usage
    and sentence variability.  Together these metrics provide a more robust
    estimate of whether the text may have been generated by a language model.

    Args:
        text: The text content to analyse.

    Returns:
        A dictionary with various metrics including readability, diversity and
        two AI likelihood estimates.
    """
    sentences = split_into_sentences(text)
    words = tokenize(text)
    word_count = len(words)
    # Ensure at least one sentence to avoid division by zero
    sentence_count = len(sentences) if sentences else 1
    unique_words = set([w.lower() for w in words])
    unique_ratio = (len(unique_words) / word_count) if word_count else 0.0
    # Count syllables across all words
    syllable_count = sum(count_syllables(w) for w in words)
    fre_score = compute_flesch_reading_ease(word_count, sentence_count, syllable_count)
    avg_sentence_length = (word_count / sentence_count) if sentence_count else 0.0
    # naive AI likelihood heuristic: emphasise low lexical diversity and mid‑range readability
    ai_likelihood = 0.0
    if word_count > 0:
        ratio_factor = max(0.0, 0.5 - unique_ratio)  # emphasise low diversity
        readability_factor = 0.0
        if 50 <= fre_score <= 90:
            readability_factor = (90 - fre_score) / 40  # scaled 0–1 over range 50–90
        ai_likelihood = min(1.0, ratio_factor + readability_factor)
    # compute more rigorous AI score using additional stylistic features
    rigorous_score = compute_rigorous_ai_score(text, {
        'word_count': word_count,
        'unique_ratio': unique_ratio,
        'flesch_reading_ease': fre_score,
        'sentence_count': sentence_count,
        'avg_sentence_length': avg_sentence_length
    }) if word_count > 0 else 0.0
    return {
        'word_count': word_count,
        'unique_word_count': len(unique_words),
        'unique_ratio': unique_ratio,
        'sentence_count': sentence_count,
        'avg_sentence_length': avg_sentence_length,
        'syllable_count': syllable_count,
        'flesch_reading_ease': fre_score,
        'ai_likelihood': ai_likelihood,
        'ai_rigorous_score': rigorous_score,
    }


def compute_rigorous_ai_score(text: str, base_metrics: Dict[str, float]) -> float:
    """Estimate AI likelihood using a combination of stylistic features.

    This detector implements a simple logistic model over several hand‑crafted
    features derived from the input text.  The goal is to capture subtle
    patterns often associated with machine‑generated prose, such as limited
    lexical variety, repetitive n‑gram structures and uniform sentence lengths.
    The resulting score is a value between 0 and 1, where higher values
    indicate a higher probability of AI generation.

    Args:
        text: The original text string (used for computing character‑level
            statistics).
        base_metrics: A dictionary containing pre‑computed metrics such as
            word_count, unique_ratio and flesch_reading_ease.

    Returns:
        A float representing the estimated AI probability.
    """
    import math
    # Tokenize and normalise words
    tokens = [w.lower() for w in tokenize(text)]
    word_count = base_metrics.get('word_count', len(tokens)) or 1
    # Average word length
    avg_word_length = sum(len(w) for w in tokens) / word_count if word_count else 0.0
    # Ratio of words with 7 or more characters (long words)
    long_word_ratio = sum(1 for w in tokens if len(w) >= 7) / word_count
    # Frequency distribution of words
    freq = {}
    for w in tokens:
        freq[w] = freq.get(w, 0) + 1
    sorted_counts = sorted(freq.values(), reverse=True)
    top_ratio = sum(sorted_counts[:5]) / word_count if sorted_counts else 0.0
    # bigram diversity and repetition ratios
    bigrams = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]
    bigram_count = len(bigrams)
    if bigram_count > 0:
        unique_bigrams = set(bigrams)
        bigram_unique_ratio = len(unique_bigrams) / bigram_count
        # number of bigrams that occur more than once
        repeated_bigrams = [bg for bg in unique_bigrams if bigrams.count(bg) > 1]
        bigram_repetition_ratio = len(repeated_bigrams) / bigram_count
    else:
        bigram_unique_ratio = 1.0
        bigram_repetition_ratio = 0.0
    # punctuation to character ratio
    total_chars = len(text)
    punctuation_count = sum(1 for ch in text if ch in string.punctuation)
    punctuation_ratio = punctuation_count / total_chars if total_chars else 0.0
    # function word ratio using a small stopword set
    stopwords = {
        'the', 'and', 'a', 'an', 'in', 'of', 'to', 'for', 'on', 'with', 'at', 'by', 'from', 'up',
        'about', 'into', 'over', 'after', 'the', 'than', 'out', 'down', 'off', 'above', 'below',
        'are', 'is', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does',
        'did', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for',
        'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',
        'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',
        'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both',
        'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own',
        'same', 'so', 'than', 'too', 'very', 'can', 'will', 'just'
    }
    function_word_count = sum(1 for w in tokens if w in stopwords)
    function_word_ratio = function_word_count / word_count
    # Sentence length variability (standard deviation)
    sentence_lengths = [len(tokenize(s)) for s in split_into_sentences(text)]
    if len(sentence_lengths) > 1:
        # Compute mean and variance manually to avoid importing statistics
        mean_len = sum(sentence_lengths) / len(sentence_lengths)
        variance = sum((l - mean_len) ** 2 for l in sentence_lengths) / (len(sentence_lengths) - 1)
        stdev_sentence_len = math.sqrt(variance)
    else:
        stdev_sentence_len = 0.0
    # Base metrics
    unique_ratio = base_metrics.get('unique_ratio', 0.0)
    flesch_score = base_metrics.get('flesch_reading_ease', 0.0)
    # Construct logistic model.  Weights are empirically chosen to amplify
    # characteristics common in AI text: low lexical diversity, high repetition,
    # moderate readability, frequent stopwords and minimal sentence variation.
    # Note: features are scaled to similar ranges; weights were tuned manually.
    z = 0.0
    z += -3.0 * unique_ratio          # lower unique ratio increases AI probability
    z += 3.0 * top_ratio              # high concentration of frequent words
    z += 4.0 * bigram_repetition_ratio # many repeated phrase pairs
    z += 1.5 * function_word_ratio    # heavy use of stopwords
    z += 0.5 * (avg_word_length / 10) # longer words modestly increase score
    z += -0.02 * flesch_score         # easier readability reduces AI probability
    z += -0.3 * stdev_sentence_len    # human writing varies sentence length more
    z += 0.5 * long_word_ratio        # high proportion of long words may indicate AI
    z += 0.5 * punctuation_ratio      # heavy punctuation usage modestly increases score
    # Apply logistic function to map z to (0,1)
    rigorous_score = 1.0 / (1.0 + math.exp(-z))
    # Clamp score to [0,1]
    if rigorous_score < 0.0:
        rigorous_score = 0.0
    elif rigorous_score > 1.0:
        rigorous_score = 1.0
    return rigorous_score


def analyse_file(path: str) -> Tuple[Dict[str, float], str]:
    """Analyse a text or code file and optionally perform an autopilot evaluation.

    If the file extension is recognised as a text format (.txt, .md, .json,
    .py, etc.), its contents are read and passed to the analysis function.
    Otherwise, a minimal analysis is returned.  An autopilot evaluation
    placeholder is performed for Python files, returning a string result.

    Args:
        path: Path to the file to analyse.

    Returns:
        A tuple containing a dictionary of analysis metrics and a string with
        autopilot evaluation feedback.
    """
    ext = os.path.splitext(path)[1].lower()
    analysis = {
        'word_count': 0,
        'unique_word_count': 0,
        'unique_ratio': 0.0,
        'sentence_count': 0,
        'avg_sentence_length': 0.0,
        'syllable_count': 0,
        'flesch_reading_ease': 0.0,
        'ai_likelihood': 0.0,
        'ai_rigorous_score': 0.0,
    }
    autopilot_result = "No autopilot evaluation performed."
    try:
        # Determine how to read file based on extension
        if ext in {'.txt', '.md', '.py', '.java', '.js', '.c', '.cpp', '.json'}:
            with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            analysis = analyse_text_content(content)
            # Attempt autopilot evaluation only for Python files
            if ext == '.py':
                autopilot_result = evaluate_python_submission(path)
        elif ext == '.pdf':
            # Attempt to extract text from PDF using the system pdftotext utility
            import subprocess
            try:
                # Use -layout to preserve approximate layout; output to stdout
                proc = subprocess.run([
                    'pdftotext', '-layout', path, '-'  # '-' outputs to stdout
                ], capture_output=True, text=True, check=True)
                content = proc.stdout
                if content:
                    analysis = analyse_text_content(content)
                else:
                    autopilot_result = "No textual content extracted from PDF."
            except FileNotFoundError:
                autopilot_result = (
                    "PDF analysis requires the 'pdftotext' utility, which is not available."
                )
            except subprocess.CalledProcessError as e:
                autopilot_result = f"Failed to extract text from PDF: {e}"
        else:
            autopilot_result = f"File type '{ext}' not supported for analysis."
    except Exception as e:
        autopilot_result = f"Error during analysis: {e}"
    return analysis, autopilot_result


def evaluate_python_submission(path: str) -> str:
    """Placeholder autopilot evaluation for Python submissions.

    This function attempts to import the submission as a module and then
    automatically calls a function named `solution` with predefined test
    inputs.  It is designed to demonstrate how an automated grading
    routine might be structured.  If no such function exists or an
    exception occurs, the function returns a message indicating that
    evaluation was not possible.

    WARNING: Executing untrusted code can be dangerous.  In a real
    deployment this should be run in a sandbox with resource limits.

    Args:
        path: Path to the Python file to import.

    Returns:
        A string describing the result of the evaluation.
    """
    import importlib.util
    import types
    module_name = os.path.splitext(os.path.basename(path))[0]
    try:
        spec = importlib.util.spec_from_file_location(module_name, path)
        if spec and spec.loader:
            module = importlib.util.module_from_spec(spec)  # type: ignore
            spec.loader.exec_module(module)  # type: ignore
        else:
            return "Unable to load the submission for evaluation."
        if hasattr(module, 'solution') and callable(getattr(module, 'solution')):
            func = getattr(module, 'solution')
            # Example test cases: the candidate's solution should sum numbers from 1 to n.
            test_cases = [5, 10, 20]
            expected_results = [15, 55, 210]
            for n, expected in zip(test_cases, expected_results):
                try:
                    result = func(n)
                except Exception as e:
                    return f"Function raised an exception for input {n}: {e}"
                if result != expected:
                    return (f"Test failed for input {n}: expected {expected}, got {result}.\n"
                            "Please ensure your 'solution(n)' function returns the sum from 1 to n.")
            return "All tests passed successfully."
        else:
            return ("No function named 'solution' was found in the submission. "
                    "Autopilot evaluation could not be performed.")
    except Exception as exc:
        return f"An error occurred during autopilot evaluation: {exc}"